{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify sys.path\n",
    "import os\n",
    "import sys\n",
    "\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "sys.path.append(project_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from platypus import NSGAII, ProcessPoolEvaluator, MaxTime\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import time\n",
    "import random \n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Dropout, InputLayer\n",
    "\n",
    "import config.config as config\n",
    "from src.data_processing import read_arff, preprocess_data\n",
    "from src.evaluation import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callback function to store the solutions from each evaluation\n",
    "def callback_function(algorithm):\n",
    "    solution_eval.append(algorithm.result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "DATA_PATH = os.path.join('..', 'data', config.DATASET_NAME)\n",
    "\n",
    "dataset = read_arff(DATA_PATH)\n",
    "df_dict = preprocess_data(dataset)\n",
    "\n",
    "train_X_timeseries, train_Y_timeseries, test_X_timeseries, test_Y_timeseries = df_dict['timeseries']\n",
    "train_X, train_Y, test_X, test_Y = df_dict['normalized']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-surrogate cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from problems.Multi_Surrogate_FS_ML import Multi_Surrogate_FS_ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load results  \n",
    "with open(f'../variables/{config.DATASET_SAVE_NAME}-dataset-cv-RF.pickle', 'rb') as f:\n",
    "    train_X_cv_RF, train_Y_cv_RF, test_X_cv_RF, test_Y_cv_RF = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "generationsPerRun = []\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    dfSolutionsMultiRF = pd.DataFrame(columns=['Run', 'Generations', 'RMSE MOEA', 'N', 'RMSE CV', 'MAE CV', 'CC CV', \n",
    "                                               'Mean RMSE CV', 'Mean MAE CV', 'Mean CC CV', \n",
    "                                               'RMSE StepsAhead', 'MAE StepsAhead', 'CC StepsAhead', \n",
    "                                               'Mean RMSE StepsAhead', 'Mean MAE StepsAhead', 'Mean CC StepsAhead',\n",
    "                                               'SelectedAttrib'])\n",
    "    \n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    problem = Multi_Surrogate_FS_ML(nVar=config.N_ATTRIB, nobjs=2, X_cv=train_X_cv_RF, Y_cv=train_Y_cv_RF, \n",
    "                                                 regex=f'../models/{config.DATASET_SAVE_NAME}-surrogate-RF-[0-9]*.pkl')\n",
    "    \n",
    "    start_time = time.time()\n",
    "    for seedRun in range(config.N_SEEDS):\n",
    "        print(\"--- Run %s ---\" % seedRun)\n",
    "        random.seed(seedRun)\n",
    "        with ProcessPoolEvaluator(config.N_JOBS) as evaluator:\n",
    "            solution_eval = []\n",
    "            algorithm = NSGAII(problem, population_size=config.POPULATION_SIZE, evaluator=evaluator)\n",
    "            algorithm.run(MaxTime(config.MAX_TIME), callback=callback_function)\n",
    "            \n",
    "            results[str(seedRun)] = algorithm.result\n",
    "            generationsPerRun.append(solution_eval)\n",
    "            \n",
    "        df = train_evaluate_ML(train_X, train_Y, algorithm.result, \n",
    "                                     RandomForestRegressor(random_state=config.SEED_VALUE), seedRun, len(solution_eval))\n",
    "        dfSolutionsMultiRF = pd.concat([dfSolutionsMultiRF, df], ignore_index=True)\n",
    "        \n",
    "    print(\"--- %s minutes ---\" % ((time.time() - start_time)/60))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results\n",
    "with open(f'../variables/{config.DATASET_SAVE_NAME}-results-MS-RF.pickle', 'wb') as f:\n",
    "     pickle.dump([dfSolutionsMultiRF], f)\n",
    "        \n",
    "with open(f'../variables/{config.DATASET_SAVE_NAME}-generations-MS-RF.pickle', 'wb') as f:\n",
    "     pickle.dump([generationsPerRun], f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from problems.Multi_Surrogate_FS_LSTM import Multi_Surrogate_FS_LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load results  \n",
    "with open(f'../variables/{config.DATASET_SAVE_NAME}-dataset-cv-LSTM.pickle', 'rb') as f:\n",
    "    train_X_cv_LSTM, train_Y_cv_LSTM, test_X_cv_LSTM, test_Y_cv_LSTM = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LSTM_model(Tx):\n",
    "    \n",
    "    model = Sequential([\n",
    "            InputLayer(shape=(1, Tx)),\n",
    "            LSTM(units=config.N_NEURONS, activation='tanh', recurrent_activation = 'sigmoid', \n",
    "                   return_sequences=True),\n",
    "            Dropout(0.2),\n",
    "            Dense(1, activation=\"linear\")\n",
    "        ])\n",
    "\n",
    "    model.compile(loss=tf.keras.losses.MeanSquaredError(), optimizer='adam',\n",
    "                  metrics=[tf.keras.metrics.RootMeanSquaredError()])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "generationsPerRun = []\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    dfSolutionsMultiLSTM = pd.DataFrame(columns=['Run', 'Generations', 'RMSE MOEA', 'N', 'RMSE CV', 'MAE CV', 'CC CV', \n",
    "                                               'Mean RMSE CV', 'Mean MAE CV', 'Mean CC CV', \n",
    "                                               'RMSE StepsAhead', 'MAE StepsAhead', 'CC StepsAhead', \n",
    "                                               'Mean RMSE StepsAhead', 'Mean MAE StepsAhead', 'Mean CC StepsAhead',\n",
    "                                               'SelectedAttrib'])\n",
    "    results = {}\n",
    "    \n",
    "    # define the problem definition\n",
    "    problem = Multi_Surrogate_FS_LSTM(nVar=config.N_ATTRIB, nobjs=2, X_cv=train_X_cv_LSTM, Y_cv=train_Y_cv_LSTM)\n",
    "    \n",
    "    # instantiate the optimization algorithm to run in parallel\n",
    "    start_time = time.time()\n",
    "    for seedRun in range(config.N_SEEDS):\n",
    "        print(\"--- Run %s ---\" % seedRun)\n",
    "        random.seed(seedRun)\n",
    "        with ProcessPoolEvaluator(config.N_JOBS) as evaluator:\n",
    "            solution_eval = []\n",
    "            algorithm = NSGAII(problem, population_size=config.POPULATION_SIZE, evaluator=evaluator)\n",
    "            algorithm.run(MaxTime(config.MAX_TIME), callback=callback_function)\n",
    "            \n",
    "            results[str(seedRun)] = algorithm.result\n",
    "            generationsPerRun.append(solution_eval)\n",
    "            \n",
    "        df = train_evaluate_LSTM(train_X, train_Y, algorithm.result, \n",
    "                                       LSTM_model, seedRun, len(solution_eval))\n",
    "        \n",
    "        dfSolutionsMultiLSTM = pd.concat([dfSolutionsMultiLSTM, df], ignore_index=True)\n",
    "        \n",
    "    print(\"--- %s minutes ---\" % ((time.time() - start_time)/60))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results\n",
    "with open(f'../variables/{config.DATASET_SAVE_NAME}-results-MS-LSTM.pickle', 'wb') as f:\n",
    "     pickle.dump([dfSolutionsMultiLSTM], f)\n",
    "        \n",
    "with open(f'../variables/{config.DATASET_SAVE_NAME}-generations-MS-LSTM.pickle', 'wb') as f:\n",
    "     pickle.dump([generationsPerRun], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from problems.Wrapper_ML import Wrapper_ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "generationsPerRun = []\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    dfSolutionsMultiRF = pd.DataFrame(columns=['Run', 'Generations', 'RMSE MOEA', 'N', 'RMSE CV', 'MAE CV', 'CC CV', \n",
    "                                               'Mean RMSE CV', 'Mean MAE CV', 'Mean CC CV', \n",
    "                                               'RMSE StepsAhead', 'MAE StepsAhead', 'CC StepsAhead', \n",
    "                                               'Mean RMSE StepsAhead', 'Mean MAE StepsAhead', 'Mean CC StepsAhead',\n",
    "                                               'SelectedAttrib'])\n",
    "    results = {}\n",
    "    \n",
    "    # define the problem definition\n",
    "    problem = Wrapper_ML(nVar=config.N_ATTRIB, nobjs=2,\n",
    "                                          train_X=train_X, train_y=train_Y, \n",
    "                                          model=RandomForestRegressor(random_state=config.SEED_VALUE))\n",
    "    \n",
    "    # instantiate the optimization algorithm to run in parallel\n",
    "    start_time = time.time()\n",
    "    for seedRun in range(config.N_SEEDS):\n",
    "        print(\"--- Run %s ---\" % seedRun)\n",
    "        random.seed(seedRun)\n",
    "        with ProcessPoolEvaluator(config.N_JOBS) as evaluator:\n",
    "            solution_eval = []\n",
    "            algorithm = NSGAII(problem, population_size=config.POPULATION_SIZE, evaluator=evaluator)\n",
    "            algorithm.run(MaxTime(config.MAX_TIME), callback=callback_function)\n",
    "            \n",
    "            results[str(seedRun)] = algorithm.result\n",
    "            generationsPerRun.append(solution_eval)\n",
    "            \n",
    "        df = train_evaluate_ML(train_X, train_Y, algorithm.result, \n",
    "                                     RandomForestRegressor(random_state=config.SEED_VALUE), seedRun, len(solution_eval))\n",
    "        dfSolutionsMultiRF = pd.concat([dfSolutionsMultiRF, df], ignore_index=True)\n",
    "        \n",
    "    print(\"--- %s minutes ---\" % ((time.time() - start_time)/60))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results\n",
    "with open(f'../variables/{config.DATASET_SAVE_NAME}-results-wrapper-RF.pickle', 'wb') as f:\n",
    "     pickle.dump([dfSolutionsMultiRF], f)\n",
    "        \n",
    "with open(f'../variables/{config.DATASET_SAVE_NAME}-generations-wrapper-RF.pickle', 'wb') as f:\n",
    "     pickle.dump([generationsPerRun], f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from problems.Wrapper_LSTM import Wrapper_LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "generationsPerRun = []\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    dfSolutionsMultiLSTM = pd.DataFrame(columns=['Run', 'Generations', 'RMSE MOEA', 'N', 'RMSE CV', 'MAE CV', 'CC CV', \n",
    "                                               'Mean RMSE CV', 'Mean MAE CV', 'Mean CC CV', \n",
    "                                               'RMSE StepsAhead', 'MAE StepsAhead', 'CC StepsAhead', \n",
    "                                               'Mean RMSE StepsAhead', 'Mean MAE StepsAhead', 'Mean CC StepsAhead',\n",
    "                                               'SelectedAttrib'])\n",
    "    results = {}\n",
    "    \n",
    "    # define the problem definition\n",
    "    problem = Wrapper_LSTM(nVar=config.N_ATTRIB, nobjs=2, train_X=train_X, train_y=train_Y)\n",
    "    \n",
    "    # instantiate the optimization algorithm to run in parallel\n",
    "    for seedRun in range(config.N_SEEDS):\n",
    "        print(\"--- Run %s ---\" % seedRun)\n",
    "        random.seed(seedRun)\n",
    "        with ProcessPoolEvaluator(config.N_JOBS) as evaluator:\n",
    "            solution_eval = []\n",
    "            start_time = time.time()\n",
    "            \n",
    "            algorithm = NSGAII(problem, population_size=config.POPULATION_SIZE, evaluator=evaluator)\n",
    "            algorithm.run(config.N_EVAL, callback=callback_function)\n",
    "            \n",
    "            results[str(seedRun)] = algorithm.result\n",
    "            generationsPerRun.append(solution_eval)\n",
    "              \n",
    "        df = train_evaluate_LSTM(train_X, train_Y, algorithm.result, \n",
    "                                       LSTM_model, seedRun, len(solution_eval))\n",
    "        \n",
    "        dfSolutionsMultiLSTM = pd.concat([dfSolutionsMultiLSTM, df], ignore_index=True)\n",
    "\n",
    "    print(\"--- %s minutes ---\" % ((time.time() - start_time)/60))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results\n",
    "with open(f'../variables/{config.DATASET_SAVE_NAME}-results-wrapper-LSTM.pickle', 'wb') as f:\n",
    "     pickle.dump([dfSolutionsMultiLSTM], f)\n",
    "        \n",
    "with open(f'../variables/{config.DATASET_SAVE_NAME}-generations-wrapper-LSTM.pickle', 'wb') as f:\n",
    "     pickle.dump([generationsPerRun], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decission making"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load results\n",
    "with open(f'../variables/{config.DATASET_SAVE_NAME}-results-MS-RF.pickle', 'rb') as f:\n",
    "    dfSolutions_multisurr_RF_WS7 = pickle.load(f)[0]\n",
    "dfSolutions_multisurr_RF_WS7['Approach'] = 'Multi-surrogate RF'\n",
    "with open(f'../variables/{config.DATASET_SAVE_NAME}-results-MS-LSTM.pickle', 'rb') as f:\n",
    "    dfSolutions_multisurr_LSTM_WS7 = pickle.load(f)[0]\n",
    "dfSolutions_multisurr_LSTM_WS7['Approach'] = 'Multi-surrogate LSTM'\n",
    "\n",
    "with open(f'../variables/{config.DATASET_SAVE_NAME}-results-wrapper-RF.pickle', 'rb') as f:\n",
    "    dfSolutions_wrapper_RF_WS7 = pickle.load(f)[0]\n",
    "dfSolutions_wrapper_RF_WS7['Approach'] = 'Wrapper RF'\n",
    "with open(f'../variables/{config.DATASET_SAVE_NAME}-results-wrapper-LSTM.pickle', 'rb') as f:\n",
    "    dfSolutions_wrapper_LSTM_WS7 = pickle.load(f)[0]\n",
    "dfSolutions_wrapper_LSTM_WS7['Approach'] = 'Wrapper LSTM'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfConcat = pd.concat([dfSolutions_multisurr_RF_WS7, dfSolutions_multisurr_LSTM_WS7, dfSolutions_wrapper_RF_WS7, dfSolutions_wrapper_LSTM_WS7], \n",
    "                     ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfH = calculate_H_CV(dfConcat, config.N_STEPS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfBestModels = dfH.loc[dfH.groupby('Approach')['H CV'].idxmin()].sort_values(by='H CV')\n",
    "dfBestModels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Best prediction models results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfHoldOut_list = []  \n",
    "\n",
    "for _, row in dfBestModels.iterrows():\n",
    "    if row['Approach'].endswith('RF'):\n",
    "        result = best_models_ML_test(\n",
    "            train_X, train_Y, test_X, test_Y, \n",
    "            row[['Approach', 'Run', 'Generations', 'RMSE MOEA', 'N', 'H CV', 'SelectedAttrib']], \n",
    "            RandomForestRegressor(random_state=config.SEED_VALUE)\n",
    "        )\n",
    "        dfHoldOut_list.append(result)\n",
    "    elif row['Approach'].endswith('LSTM'):\n",
    "        result = best_models_LSTM_test(\n",
    "            train_X, train_Y, test_X, test_Y, \n",
    "            row[['Approach', 'Run', 'Generations', 'RMSE MOEA', 'N', 'H CV', 'SelectedAttrib']], \n",
    "            LSTM_model\n",
    "        )\n",
    "        dfHoldOut_list.append(result)\n",
    "\n",
    "dfHoldOut = pd.DataFrame(dfHoldOut_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H_train, H_test = calculate_H_train_test(dfHoldOut, config.N_STEPS)\n",
    "dfHoldOut['H Train'] = H_train\n",
    "dfHoldOut['H Test'] = H_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfHoldOut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "calm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
